{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"evaluacion\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as Chinook.db\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Open a local file in binary write mode\n",
    "    with open(\"Chinook.db\", \"wb\") as file:\n",
    "        # Write the content of the response (the file) to the local file\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded and saved as Chinook.db\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM Artist LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o\",temperature=0)\n",
    "experiment_prefix=\"sql-agent-gpt4o\"\n",
    "metadata = \"Chinook, gpt-4o base-case-agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import tool\n",
    "\n",
    "# SQL toolkit\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "# Query checking\n",
    "query_check_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "Double check the SQLite query for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "\n",
    "# If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\n",
    "# Execute the correct query with the appropriate tool.\"\"\"\n",
    "query_check_prompt = ChatPromptTemplate.from_messages([(\"system\", query_check_system),(\"user\", \"{query}\")])\n",
    "query_check = query_check_prompt | llm\n",
    "\n",
    "@tool\n",
    "def check_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to double check if your query is correct before executing it.\n",
    "    \"\"\"\n",
    "    return query_check.invoke({\"query\": query}).content\n",
    "\n",
    "# Query result checking\n",
    "query_result_check_system = \"\"\"You are grading the result of a SQL query from a DB.\n",
    "- Check that the result is not empty.\n",
    "- If it is empty, instruct the system to re-try!\"\"\"\n",
    "query_result_check_prompt = ChatPromptTemplate.from_messages([(\"system\", query_result_check_system),(\"user\", \"{query_result}\")])\n",
    "query_result_check = query_result_check_prompt | llm\n",
    "\n",
    "@tool\n",
    "def check_result(query_result: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to check the query result from the database to confirm it is not empty and is relevant.\n",
    "    \"\"\"\n",
    "    return query_result_check.invoke({\"query_result\": query_result}).content\n",
    "\n",
    "tools.append(check_query_tool)\n",
    "tools.append(check_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "# Assistant\n",
    "class Assistant:\n",
    "\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            # Append to state\n",
    "            state = {**state}\n",
    "            # Invoke the tool-calling LLM\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If it is a tool call -> response is valid\n",
    "            # If it has meaninful text -> response is valid\n",
    "            # Otherwise, we re-prompt it b/c response is not meaninful\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "# Assistant runnable\n",
    "query_gen_system = \"\"\"\n",
    "ROLE:\n",
    "You are an agent designed to interact with a SQL database. You have access to tools for interacting with the database.\n",
    "GOAL:\n",
    "Given an input question, create a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n",
    "INSTRUCTIONS:\n",
    "- Only use the below tools for the following operations.\n",
    "- Only use the information returned by the below tools to construct your final answer.\n",
    "- To start you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step.\n",
    "- Then you should query the schema of the most relevant tables.\n",
    "- Write your query based upon the schema of the tables. You MUST double check your query before executing it.\n",
    "- Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
    "- You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "- Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "- If you get an error while executing a query, rewrite the query and try again.\n",
    "- If the query returns a result, use check_result tool to check the query result.\n",
    "- If the query result result is empty, think about the table schema, rewrite the query, and try again.\n",
    "- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"\n",
    "\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages([(\"system\", query_gen_system),(\"placeholder\", \"{messages}\")])\n",
    "assistant_runnable = query_gen_prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either agent can decide to end\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def router(state) -> Literal[\"call_tool\", \"__end__\"]:\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        # The previous agent is invoking a tool\n",
    "        return \"call_tool\"\n",
    "    \n",
    "    return \"__end__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
    "builder.add_node(\"call_tool\", create_tool_node_with_fallback(tools))\n",
    "# builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.set_entry_point(\"assistant\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    router,\n",
    "    {\"assistant\": \"assistant\", \"call_tool\": \"call_tool\", \"__end__\": END},\n",
    ")\n",
    "# builder.add_conditional_edges(\n",
    "#     \"assistant\",\n",
    "#     # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "#     # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    # tools_condition,\n",
    "#     # \"tools\" calls one of our tools. END causes the graph to terminate (and respond to the user)\n",
    "#     {\"tools\": \"tools\", END: END},\n",
    "# )\n",
    "builder.add_edge(\"call_tool\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Which country's customers spent the most? And how much did they spend?\",\n",
    "             \"How many albums does the artist Led Zeppelin have?\",\n",
    "             \"What was the most purchased track of 2017?\",\n",
    "             \"Which sales agent made the most in sales in 2009?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Checkpoints are accessed by thread_id\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: thread_id,\n\u001b[0;32m     13\u001b[0m     }\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m msg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, questions[\u001b[38;5;241m0\u001b[39m])}\n\u001b[1;32m---> 17\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m messages[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mf:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1448\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1450\u001b[0m     config,\n\u001b[0;32m   1451\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1452\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1453\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[0;32m   1454\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1455\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1456\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1458\u001b[0m ):\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1460\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mf:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:949\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    943\u001b[0m end_time \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m futures:\n\u001b[1;32m--> 949\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[0;32m    957\u001b[0m         task \u001b[38;5;241m=\u001b[39m futures\u001b[38;5;241m.\u001b[39mpop(fut)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(fs, timeout, return_when)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[0;32m    305\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "questions = [\"Which country's customers spent the most? And how much did they spend?\",\n",
    "           ]\n",
    "\n",
    "\n",
    "import uuid\n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "msg = {\"messages\": (\"user\", questions[0])}\n",
    "messages = graph.invoke(msg,config)\n",
    "messages['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hola\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola! ¿En qué puedo ayudarte hoy? Si tienes alguna pregunta o necesitas información de una base de datos SQL, estaré encantado de asistirte.\n"
     ]
    }
   ],
   "source": [
    "## Stream\n",
    "questions = [\"Hola\",\n",
    "           ]\n",
    "\n",
    "import uuid\n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": (\"user\", questions[0])}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Which country's customers spent the most? And how much did they spend?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_list_tables (call_M306ga96rDD8uaq4v0LDlzCm)\n",
      " Call ID: call_M306ga96rDD8uaq4v0LDlzCm\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_list_tables\n",
      "\n",
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_schema (call_hPfTm1NI4Q5ZSDZknwzFofKY)\n",
      " Call ID: call_hPfTm1NI4Q5ZSDZknwzFofKY\n",
      "  Args:\n",
      "    table_names: Customer, Invoice\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_schema\n",
      "\n",
      "\n",
      "CREATE TABLE \"Customer\" (\n",
      "\t\"CustomerId\" INTEGER NOT NULL, \n",
      "\t\"FirstName\" NVARCHAR(40) NOT NULL, \n",
      "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
      "\t\"Company\" NVARCHAR(80), \n",
      "\t\"Address\" NVARCHAR(70), \n",
      "\t\"City\" NVARCHAR(40), \n",
      "\t\"State\" NVARCHAR(40), \n",
      "\t\"Country\" NVARCHAR(40), \n",
      "\t\"PostalCode\" NVARCHAR(10), \n",
      "\t\"Phone\" NVARCHAR(24), \n",
      "\t\"Fax\" NVARCHAR(24), \n",
      "\t\"Email\" NVARCHAR(60) NOT NULL, \n",
      "\t\"SupportRepId\" INTEGER, \n",
      "\tPRIMARY KEY (\"CustomerId\"), \n",
      "\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Customer table:\n",
      "CustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId\n",
      "1\tLuís\tGonçalves\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\tAv. Brigadeiro Faria Lima, 2170\tSão José dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t3\n",
      "2\tLeonie\tKöhler\tNone\tTheodor-Heuss-Straße 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t5\n",
      "3\tFrançois\tTremblay\tNone\t1498 rue Bélanger\tMontréal\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE \"Invoice\" (\n",
      "\t\"InvoiceId\" INTEGER NOT NULL, \n",
      "\t\"CustomerId\" INTEGER NOT NULL, \n",
      "\t\"InvoiceDate\" DATETIME NOT NULL, \n",
      "\t\"BillingAddress\" NVARCHAR(70), \n",
      "\t\"BillingCity\" NVARCHAR(40), \n",
      "\t\"BillingState\" NVARCHAR(40), \n",
      "\t\"BillingCountry\" NVARCHAR(40), \n",
      "\t\"BillingPostalCode\" NVARCHAR(10), \n",
      "\t\"Total\" NUMERIC(10, 2) NOT NULL, \n",
      "\t ... (truncated)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_query_checker (call_J3z69teDeN9RQNJiP82U7auy)\n",
      " Call ID: call_J3z69teDeN9RQNJiP82U7auy\n",
      "  Args:\n",
      "    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_query_checker\n",
      "\n",
      "```sql\n",
      "SELECT c.Country, SUM(i.Total) as TotalSpent \n",
      "FROM Customer c \n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId \n",
      "GROUP BY c.Country \n",
      "ORDER BY TotalSpent DESC \n",
      "LIMIT 1;\n",
      "```\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_query (call_rITLVEi9kvlSuoeUXRMI2K9J)\n",
      " Call ID: call_rITLVEi9kvlSuoeUXRMI2K9J\n",
      "  Args:\n",
      "    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_query\n",
      "\n",
      "[('USA', 523.0600000000003)]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The country whose customers spent the most is the USA, with a total spending of $523.06.\n"
     ]
    }
   ],
   "source": [
    "## Invoke\n",
    "\n",
    "# questions = [\"Which country's customers spent the most? And how much did they spend?\",\n",
    "#            ]\n",
    "\n",
    "\n",
    "# import uuid\n",
    "# _printed = set()\n",
    "# thread_id = str(uuid.uuid4())\n",
    "\n",
    "# config = {\n",
    "#     \"configurable\": {\n",
    "#         # Checkpoints are accessed by thread_id\n",
    "#         \"thread_id\": thread_id,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# msg = {\"messages\": (\"user\", questions[0])}\n",
    "# messages = graph.invoke(msg,config)\n",
    "# messages['messages'][-1].content\n",
    "\n",
    "# ## Stream\n",
    "# import uuid\n",
    "# _printed = set()\n",
    "# thread_id = str(uuid.uuid4())\n",
    "\n",
    "# config = {\n",
    "#     \"configurable\": {\n",
    "#         # Checkpoints are accessed by thread_id\n",
    "#         \"thread_id\": thread_id,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# events = graph.stream(\n",
    "#     {\"messages\": (\"user\", questions[0])}, config, stream_mode=\"values\"\n",
    "# )\n",
    "# for event in events:\n",
    "#     _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "# Create a dataset\n",
    "examples = [\n",
    "    (\"Which country's customers spent the most? And how much did they spend?\", \n",
    "     \"The country whose customers spent the most is the USA, with a total expenditure of $523.06\"),\n",
    "    (\"What was the most purchased track of 2013?\", \"The most purchased track of 2013 was Hot Girl.\"),\n",
    "    (\"How many albums does the artist Led Zeppelin have?\",\"Led Zeppelin has 14 albums\"),\n",
    "    (\"What is the total price for the album “Big Ones”?\",\"The total price for the album 'Big Ones' is 14.85\"),\n",
    "    (\"Which sales agent made the most in sales in 2009?\", \"Steve Johnson made the most sales in 2009\"),\n",
    "]\n",
    "\n",
    "dataset_name = \"SQL Agent Response\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sql_agent_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    msg = {\"messages\": (\"user\", example[\"input\"])}\n",
    "    messages = graph.invoke(msg, config)\n",
    "    return {\"response\": messages['messages'][-1].content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    reference = example.outputs[\"output\"]\n",
    "    prediction = run.outputs[\"response\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sql-agent-gpt4o-response-v-reference-0db72218' at:\n",
      "https://smith.langchain.com/o/67b2dd2a-05be-57ef-a75c-c435ae5d80f0/datasets/2c57c7ee-004f-43cc-bf41-724540ba9f78/compare?selectedSessions=0d803330-7a70-44ce-808b-ad36b3f6bf56\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "0it [00:00, ?it/s]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29958, Requested 1448. Please try again in 2.812s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29792, Requested 988. Please try again in 1.56s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29899, Requested 969. Please try again in 1.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29609, Requested 1833. Please try again in 2.884s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29393, Requested 1054. Please try again in 894ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29469, Requested 988. Please try again in 914ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29318, Requested 1153. Please try again in 941ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29875, Requested 1130. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 28935, Requested 1339. Please try again in 548ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29717, Requested 1314. Please try again in 2.062s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29694, Requested 1300. Please try again in 1.988s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29705, Requested 1327. Please try again in 2.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29410, Requested 1624. Please try again in 2.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MINsx6SaBshCY7YXVFk4x9gd on tokens per min (TPM): Limit 30000, Used 29435, Requested 1581. Please try again in 2.032s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 050c93f2-b08e-4615-bf93-651af15c4a23: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "1it [00:21, 21.14s/it]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 3d7d2c3e-3523-43be-88f0-696a7d895dce: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 7ddfb09d-1728-4941-ab65-983cab2c1902: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run fb021cc5-5fbd-489a-a166-a1db6386bb0b: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 1ba624ea-814e-4ecb-bda1-4bb3f646c897: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "5it [00:21,  3.17s/it]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 013d42ca-8e68-4462-bdef-a62bfe73ded6: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 410353a1-df87-473c-bea7-a6072afa35c3: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run a9709edd-1280-4fa2-8fac-64920d783ce1: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 05eece6a-cda5-451b-ade3-ae962b631d22: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "9it [00:21,  1.45s/it]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run e430b9c4-bfd7-4912-bb1a-8c3237ad5e38: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 108a4ec3-78a7-4c3d-99f0-fa93674a1082: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 25e83b74-ec8d-4672-a90b-13088568c2e7: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 4c462ff6-0666-4d1e-8607-6caa5b798a08: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "13it [00:21,  1.21it/s]Error running evaluator <DynamicRunEvaluator answer_evaluator> on run cac32955-b48f-4c04-a19f-795259b7a093: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"f:\\GitHub\\agent_with_memory\\myenv\\lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_11268\\2621053133.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_sql_agent_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=experiment_prefix + \"-response-v-reference\",\n",
    "    num_repetitions=3,\n",
    "    metadata={\"version\": metadata},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
